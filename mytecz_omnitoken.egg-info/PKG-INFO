Metadata-Version: 2.4
Name: mytecz-omnitoken
Version: 0.1.0
Summary: Universal tokenizer with modular architecture supporting multiple tokenization strategies
Home-page: https://github.com/mytecz/omnitoken
Author: MyTecZ
Author-email: Kalyana Krishna Kondapalli <kalyanakondapalli@gmail.com>
Maintainer-email: Kalyana Krishna Kondapalli <kalyanakondapalli@gmail.com>
License: MIT License
        
        Copyright (c) 2025 MyTecZ
        
        Permission is hereby granted, free of charge, to any person obtaining a copy
        of this software and associated documentation files (the "Software"), to deal
        in the Software without restriction, including without limitation the rights
        to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
        copies of the Software, and to permit persons to whom the Software is
        furnished to do so, subject to the following conditions:
        
        The above copyright notice and this permission notice shall be included in all
        copies or substantial portions of the Software.
        
        THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
        IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
        FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
        AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
        LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
        OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
        SOFTWARE.
Project-URL: Homepage, https://github.com/mytecz/omnitoken
Project-URL: Documentation, https://mytecz-omnitoken.readthedocs.io/
Project-URL: Repository, https://github.com/mytecz/omnitoken.git
Project-URL: Bug Tracker, https://github.com/mytecz/omnitoken/issues
Project-URL: Changelog, https://github.com/mytecz/omnitoken/blob/main/CHANGELOG.md
Keywords: tokenizer,nlp,natural-language-processing,bpe,wordpiece,sentencepiece,subword,machine-learning,text-processing,unicode,multilingual,hybrid-tokenization
Classifier: Development Status :: 4 - Beta
Classifier: Intended Audience :: Developers
Classifier: Intended Audience :: Science/Research
Classifier: License :: OSI Approved :: MIT License
Classifier: Operating System :: OS Independent
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Classifier: Topic :: Software Development :: Libraries :: Python Modules
Classifier: Topic :: Text Processing :: Linguistic
Classifier: Typing :: Typed
Requires-Python: >=3.8
Description-Content-Type: text/markdown
License-File: LICENSE
Provides-Extra: dev
Requires-Dist: pytest>=7.0; extra == "dev"
Requires-Dist: pytest-cov>=4.0; extra == "dev"
Requires-Dist: black>=23.0; extra == "dev"
Requires-Dist: ruff>=0.1.0; extra == "dev"
Requires-Dist: mypy>=1.0; extra == "dev"
Requires-Dist: pre-commit>=3.0; extra == "dev"
Provides-Extra: test
Requires-Dist: pytest>=7.0; extra == "test"
Requires-Dist: pytest-cov>=4.0; extra == "test"
Requires-Dist: pytest-xdist>=3.0; extra == "test"
Provides-Extra: examples
Requires-Dist: jupyter>=1.0; extra == "examples"
Requires-Dist: matplotlib>=3.5; extra == "examples"
Requires-Dist: pandas>=1.5; extra == "examples"
Requires-Dist: rich>=13.0; extra == "examples"
Provides-Extra: performance
Requires-Dist: numpy>=1.24; extra == "performance"
Requires-Dist: cython>=3.0; extra == "performance"
Provides-Extra: all
Requires-Dist: mytecz-omnitoken[dev,examples,performance,test]; extra == "all"
Dynamic: author
Dynamic: home-page
Dynamic: license-file
Dynamic: requires-python

# MyTecZ OmniToken ğŸš€

**Universal Tokenizer with Modular Architecture**

[![Python Version](https://img.shields.io/badge/python-3.10%2B-blue.svg)](https://python.org)
[![PyPI](https://img.shields.io/pypi/v/mytecz-omnitoken.svg)](https://pypi.org/project/mytecz-omnitoken/)
[![Build](https://github.com/mytecz/omnitoken/actions/workflows/publish.yml/badge.svg)](https://github.com/mytecz/omnitoken/actions)
[![Coverage](https://img.shields.io/badge/coverage-unknown-lightgrey.svg)](#) <!-- Replace with real badge once coverage upload enabled -->
[![License](https://img.shields.io/badge/license-MIT-green.svg)](LICENSE)
[![Status](https://img.shields.io/badge/status-beta-orange.svg)](https://github.com/mytecz/omnitoken)

## ğŸ¯ Overview

MyTecZ OmniToken is a universal tokenizer designed to overcome limitations in existing tokenization solutions. It provides a **modular architecture** that supports multiple tokenization strategies and input formats, making it ideal for diverse NLP applications.

### âœ¨ Key Features

- **ğŸ”§ Modular Architecture**: Plug-and-play tokenization strategies
- **ğŸ“ Multiple Input Formats**: Files, JSON, raw strings, mixed data
- **ğŸŒ Unicode & Emoji Support**: Full international character support
- **ğŸ”„ Deterministic & Reversible**: Reliable encode/decode operations
- **ğŸ§ª Experimental Hybrid Mode**: Adaptive tokenization strategies
- **ğŸ“Š Built-in Visualizer**: Debug and analyze tokenization results
- **âš¡ Performance Optimized**: Efficient training and inference
- **ğŸ›ï¸ Configurable**: Extensive customization options

### ğŸ› ï¸ Supported Tokenization Methods

- **Character-based**: Fine-grained character-level tokenization
- **BPE (Byte Pair Encoding)**: Subword tokenization with merge operations
- **WordPiece**: BERT-style tokenization with ## prefixes
- **SentencePiece**: Language-independent subword tokenization
- **Hybrid**: Experimental adaptive tokenization combining multiple strategies

## ğŸš€ Quick Start

### Installation

```bash
pip install mytecz-omnitoken
```

### Basic Usage

```python
from omnitoken import OmniToken

# Create tokenizer with your preferred method
tokenizer = OmniToken(method="hybrid", vocab_size=10000)

# Train on various input formats
training_data = [
    "Hello world! This is sample text.",
    {"text": "JSON input is supported"},
    "path/to/your/textfile.txt"  # File paths work too
]

tokenizer.fit(training_data)

# Encode text to tokens
text = "Hello world ğŸ‘‹! Unicode and emojis work perfectly ğŸš€"
tokens = tokenizer.encode(text)
print(f"Tokens: {tokens}")

# Decode back to text
decoded = tokenizer.decode(tokens)
print(f"Decoded: {decoded}")
```

### Advanced Example

```python
from omnitoken import OmniToken
from omnitoken.utils import TokenVisualizer

# Configure tokenizer with custom settings
tokenizer = OmniToken(
    method="bpe",
    vocab_size=5000,
    min_frequency=2,
    special_tokens=["[CUSTOM]", "[SPECIAL]"]
)

# Train with mixed data sources
mixed_data = [
    "Regular text for training",
    "data/corpus.txt",  # File input
    {"documents": ["Doc 1", "Doc 2"]},  # JSON input
    ["List", "of", "strings"]  # List input
]

tokenizer.fit(mixed_data)

# Analyze tokenization
test_text = "Analyze this tokenization example!"
tokens = tokenizer._tokenizer.tokenize(test_text)
token_ids = tokenizer.encode(test_text)

# Visualize results
visualization = TokenVisualizer.visualize_tokens(test_text, tokens, token_ids)
print(visualization)
```

### Frequency & Vocabulary Introspection

After training you can inspect vocabulary size and token frequency distribution:

```python
tok = OmniToken(method="bpe", vocab_size=500)
tok.fit(["Some training text", "More tokens here", "Text text text"])
vocab = tok.get_vocab()              # token -> id
freqs = tok.get_token_frequencies()  # token -> observed count
print(len(vocab), "vocab items", len(freqs), "with frequencies tracked")
print(sorted(tok.get_token_frequencies().items(), key=lambda x: -x[1])[:10])
```

You can also pass the alias `mode=` instead of `method=` (e.g. `OmniToken(mode="wordpiece")`).

## ğŸ“š Documentation

### Tokenization Methods

#### BPE (Byte Pair Encoding)
```python
tokenizer = OmniToken(method="bpe", vocab_size=10000)
```
- Learns merge operations for frequent character pairs
- Good for handling out-of-vocabulary words
- Deterministic and efficient

#### WordPiece
```python
tokenizer = OmniToken(method="wordpiece", vocab_size=10000)
```
- BERT-style tokenization with ## continuation prefixes
- Greedy longest-match-first algorithm
- Excellent for transformer models

#### SentencePiece
```python
tokenizer = OmniToken(method="sentencepiece", vocab_size=10000)
```
- Language-independent tokenization
- Treats text as raw character sequence
- Robust Unicode handling

#### Hybrid (Experimental)
```python
tokenizer = OmniToken(method="hybrid", vocab_size=10000)
```
- Combines multiple strategies adaptively
- Analyzes text characteristics to choose optimal approach
- Best for diverse content types

### Input Format Support

#### String Input
```python
tokenizer.fit("Simple string input for training")
```

#### File Input
```python
tokenizer.fit("path/to/textfile.txt")
tokenizer.fit(["file1.txt", "file2.txt", "file3.txt"])
```

#### JSON Input
```python
data = {
    "texts": ["Text 1", "Text 2"],
    "metadata": "Additional content"
}
tokenizer.fit(data)
```

#### Mixed Input
```python
mixed = [
    "Direct string",
    "data/file.txt",
    {"json": "object"},
    ["list", "of", "items"]
]
tokenizer.fit(mixed)
```

## ğŸ§ª Examples

Explore the `/examples` directory for comprehensive usage examples:

- **`example_basic.py`**: Basic usage and method comparison
- **`example_json_file_input.py`**: Advanced input format handling

Run examples:
```bash
cd examples
python example_basic.py
python example_json_file_input.py
```

## ğŸ§ª Testing

Run the comprehensive test suite:

```bash
# Basic functionality tests
python -m pytest tests/test_basic.py -v

# Round-trip tokenization tests  
python -m pytest tests/test_reverse_tokenization.py -v

# Run all tests
python -m pytest tests/ -v
```

### Test Coverage

Our test suite covers:
- âœ… Round-trip encode/decode accuracy
- âœ… Unicode and emoji handling
- âœ… All input format types
- âœ… Edge cases and error conditions
- âœ… Performance benchmarks
- âœ… Deterministic behavior
- âœ… Cross-method consistency

## âš™ï¸ Configuration Options

### TokenizerConfig Parameters

```python
from omnitoken import OmniToken
from omnitoken.tokenizer_base import TokenizerConfig

config = TokenizerConfig(
    vocab_size=10000,           # Maximum vocabulary size
    min_frequency=2,            # Minimum token frequency
    special_tokens=["[MASK]"],  # Custom special tokens
    unk_token="[UNK]",         # Unknown token
    pad_token="[PAD]",         # Padding token
    case_sensitive=True,        # Case sensitivity
    max_token_length=100       # Maximum token length
)

tokenizer = OmniToken(method="bpe", config=config)
```

### Method-Specific Options

#### BPE Options
```python
tokenizer = OmniToken(
    method="bpe",
    vocab_size=10000,
    dropout=0.1,                # BPE dropout for regularization
    end_of_word_suffix="</w>"   # End-of-word marker
)
```

#### WordPiece Options
```python
tokenizer = OmniToken(
    method="wordpiece",
    vocab_size=10000,
    continuation_prefix="##",    # Subword continuation prefix
    do_lower_case=True,         # Lowercase normalization
    max_input_chars_per_word=100 # Max characters per word
)
```

#### Hybrid Options
```python
tokenizer = OmniToken(
    method="hybrid",
    vocab_size=10000,
    char_ratio=0.3,             # Character vocab ratio
    word_ratio=0.4,             # Word vocab ratio  
    subword_ratio=0.3,          # Subword vocab ratio
    adaptive_mode=True          # Enable adaptive strategy selection
)
```

## ğŸ” Visualization and Analysis

### Token Visualization

```python
from omnitoken.utils import TokenVisualizer

# Visualize tokenization
text = "Example text with emojis ğŸ¯"
tokens = tokenizer._tokenizer.tokenize(text)
token_ids = tokenizer.encode(text)

viz = TokenVisualizer.visualize_tokens(text, tokens, token_ids)
print(viz)
```

### Method Comparison

```python
# Compare different tokenization methods
tokenizations = {
    "BPE": bpe_tokenizer.tokenize(text),
    "WordPiece": wp_tokenizer.tokenize(text),
    "Hybrid": hybrid_tokenizer.tokenize(text)
}

comparison = TokenVisualizer.compare_tokenizations(text, tokenizations)
print(comparison)
```

### Vocabulary Statistics

```python
vocab = tokenizer._tokenizer.get_vocab()
stats = TokenVisualizer.show_vocabulary_stats(vocab)
print(stats)
```

## ğŸ¯ Use Cases

### Natural Language Processing
- Text preprocessing for transformer models
- Multi-language document processing
- Social media content analysis

### Code Analysis
- Programming language tokenization
- Code documentation processing
- Technical text analysis

### Content Processing
- Web scraping and text extraction
- Document indexing and search
- Content recommendation systems

### Research and Development
- Tokenization algorithm research
- Comparative analysis studies
- Custom tokenization strategies

## ğŸ—ï¸ Architecture

```
mytecz_omnitoken/
â”œâ”€â”€ omnitoken/
â”‚   â”œâ”€â”€ __init__.py              # Main package interface
â”‚   â”œâ”€â”€ tokenizer_base.py        # Abstract base class
â”‚   â”œâ”€â”€ tokenizer_bpe.py         # BPE implementation
â”‚   â”œâ”€â”€ tokenizer_wordpiece.py   # WordPiece implementation
â”‚   â”œâ”€â”€ tokenizer_sentencepiece.py # SentencePiece implementation
â”‚   â”œâ”€â”€ tokenizer_hybrid.py      # Hybrid tokenizer
â”‚   â”œâ”€â”€ trainer.py               # Training algorithms
â”‚   â””â”€â”€ utils.py                 # Utilities and visualization
â”œâ”€â”€ examples/                    # Usage examples
â”œâ”€â”€ tests/                       # Test suite
â””â”€â”€ README.md                    # This file
```

## ğŸ¤ Contributing

We welcome contributions! Here's how you can help:

1. **ğŸ› Report Bugs**: Open issues for bugs or unexpected behavior
2. **ğŸ’¡ Feature Requests**: Suggest new features or improvements
3. **ğŸ“ Documentation**: Help improve documentation and examples
4. **ğŸ§ª Testing**: Add test cases for edge cases
5. **ğŸ”§ Code**: Submit pull requests with bug fixes or features

### Development Setup

```bash
# Clone the repository
git clone https://github.com/mytecz/omnitoken.git
cd omnitoken

# Install in development mode
pip install -e .[dev]

# Run tests
python -m pytest tests/ -v

# Format code
black omnitoken/ tests/ examples/

# Type checking
mypy omnitoken/
```

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Inspired by research in subword tokenization algorithms
- Built with love for the NLP and ML community
- Special thanks to contributors and beta testers

## ğŸ“ Contact

- **GitHub**: [https://github.com/mytecz/omnitoken](https://github.com/mytecz/omnitoken)
- **Email**: contact@mytecz.com
- **Issues**: [https://github.com/mytecz/omnitoken/issues](https://github.com/mytecz/omnitoken/issues)

---

**Made with â¤ï¸ by MyTecZ**

*Empowering NLP with Universal Tokenization* ğŸš€
